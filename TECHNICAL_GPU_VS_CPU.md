# Technical Explanation: GPU vs CPU Model Loading

## The Question

**"Why does GPU mode say it only needs adapters when it actually downloads the base model too?"**

This is a great question that highlights an important distinction between **download size** and **runtime memory usage**.

## TL;DR

- **Both modes** download ~22 GB (base model + adapters)
- **GPU mode** uses 4-bit quantization ‚Üí only ~3.5 GB VRAM
- **CPU mode** uses full precision ‚Üí ~16-20 GB RAM
- **GPU mode appears smaller** because base model is often already cached

## Detailed Explanation

### What Actually Gets Downloaded

#### GPU Mode
```
Base Model:     ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/
                Size: ~14 GB (safetensors files)

LoRA Adapters:  ~/.cache/huggingface/hub/models--ttennant--qwen2.5-7b-medical-lora/
                Size: ~8 GB (adapter weights)

Total:          ~22 GB on disk
```

#### CPU Mode
```
Base Model:     ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/
                Size: ~14 GB (safetensors files)

LoRA Adapters:  ~/.cache/huggingface/hub/models--ttennant--qwen2.5-7b-medical-lora/
                Size: ~8 GB (adapter weights)

Total:          ~22 GB on disk
```

**They're identical on disk!** So why do we say GPU mode is "8 GB"?

### The Magic: HuggingFace Cache

HuggingFace caches models in `~/.cache/huggingface/hub/`. This cache is **shared across all applications and models**.

**Scenario 1: Fresh Installation (No Qwen Models)**
- GPU Mode: Downloads 14 GB (base) + 8 GB (adapters) = **22 GB total**
- CPU Mode: Downloads 14 GB (base) + 8 GB (adapters) = **22 GB total**

**Scenario 2: You Already Have Qwen 2.5 7B Cached**
- GPU Mode: Downloads 0 GB (cached) + 8 GB (adapters) = **8 GB download**
- CPU Mode: Downloads 0 GB (cached) + 8 GB (adapters) = **8 GB download**

So when we say "8 GB", we're being optimistic that users might already have the base model cached!

### The Real Difference: Runtime Memory Usage

This is where GPU mode shines:

#### GPU Mode (with `unsloth` + 4-bit quantization)

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="ttennant/qwen2.5-7b-medical-lora",
    load_in_4bit=True,  # ‚Üê The magic happens here
)
```

**What happens:**
1. Reads base model from disk (~14 GB files)
2. **Loads as 4-bit integers** instead of 32-bit floats
3. 7B parameters √ó 4 bits = ~3.5 GB VRAM
4. Applies LoRA adapters dynamically (minimal overhead)
5. **Total VRAM: ~3.5-4 GB**

**Memory Reduction:**
- Original: 7B params √ó 32 bits = 28 GB (FP32)
- With 4-bit: 7B params √ó 4 bits = 3.5 GB
- **Reduction: 87.5%!**

#### CPU Mode (with `transformers` + `peft`)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.float32,  # ‚Üê Full precision
)
peft_model = PeftModel.from_pretrained(base_model, "ttennant/qwen2.5-7b-medical-lora")
merged_model = peft_model.merge_and_unload()
```

**What happens:**
1. Reads base model from disk (~14 GB files)
2. **Loads as 32-bit floats** (full precision)
3. 7B parameters √ó 32 bits = 28 GB... but Python overhead reduces this to ~14 GB RAM
4. Loads LoRA adapters (~2 GB in memory)
5. Merges adapters into base model (creates new merged weights)
6. **Total RAM: ~16-20 GB**

### Why Not Quantize on CPU?

Good question! You can, but:

1. **CPU quantization is slower** - No dedicated hardware (like Tensor Cores on GPU)
2. **Libraries assume GPU** - Most 4-bit/8-bit quant tools are GPU-optimized
3. **RAM is cheaper than VRAM** - Most systems have 32+ GB RAM but only 8-12 GB VRAM
4. **Accuracy matters more** - When already slow, might as well use full precision

### Comparison Table

| Aspect | GPU Mode | CPU Mode |
|--------|----------|----------|
| **Download (Fresh)** | ~22 GB | ~22 GB |
| **Download (Cached Base)** | ~8 GB | ~8 GB |
| **On-Disk Storage** | ~22 GB | ~22 GB |
| **Runtime Memory** | ~3.5 GB VRAM | ~16-20 GB RAM |
| **Precision** | 4-bit INT | 32-bit FP |
| **Speed** | Fast (5-15s) | Slow (60-300s) |
| **Quality** | ~98% of full | 100% (full) |

### The Code Difference

#### GPU Mode (local_model.py)
```python
# Unsloth handles everything automatically
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_REPO_ID,  # Points to LoRA adapter repo
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # Auto-detect best dtype
    load_in_4bit=True,  # Enable 4-bit quantization
)

# Behind the scenes, unsloth:
# 1. Finds base model from adapter config
# 2. Downloads base model if not cached
# 3. Loads base model in 4-bit format
# 4. Applies LoRA adapters
# 5. Returns merged model (in-memory only)
```

#### CPU Mode (local_model.py)
```python
# Manual process
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    device_map="cpu",
    torch_dtype=torch.float32,
)

# Then apply adapters
peft_model = PeftModel.from_pretrained(base_model, MODEL_REPO_ID)

# Merge for faster inference
merged_model = peft_model.merge_and_unload()
```

### Why Say "8 GB" for GPU Mode?

In our documentation, we simplified this to "~8 GB" for GPU mode because:

1. **User Expectation**: Most users installing a "medical LoRA" expect to download the adapters
2. **Cache Optimization**: Qwen 2.5 7B is popular; many users may have it cached
3. **Incremental Cost**: If you already have base model, only pay for adapters
4. **Marketing**: "8 GB" sounds better than "22 GB" üòÖ

But you're right - we should be more accurate! The updated docs now say:
- **"~8-22 GB (varies based on HuggingFace cache)"**

### Practical Implications

**For Users:**
- First Qwen model install: Expect full 22 GB download
- Already have Qwen models: Only 8 GB additional
- GPU mode is ALWAYS worth it if you have CUDA (faster + less memory)

**For Developers:**
- Cache directory: `~/.cache/huggingface/hub/`
- Check cache: `ls ~/.cache/huggingface/hub/ | grep -i qwen`
- Clear cache: `rm -rf ~/.cache/huggingface/hub/models--Qwen--*`

### Why Quantization Works

**4-bit Quantization** means representing each weight with 4 bits instead of 32:

```
Original (FP32):  0.123456789  (32 bits, high precision)
Quantized (INT4): 0.125        (4 bits, lower precision)
```

For language models:
- **Weights are redundant** - Many similar values
- **Small errors are okay** - Model is robust to noise
- **Lookup tables** - 4-bit values map to 16 discrete levels
- **Calibration** - Carefully chosen ranges preserve accuracy

Result: ~98% of full precision accuracy with 87.5% less memory!

### Conclusion

Both GPU and CPU modes download the same files (~22 GB total), but:

- **GPU mode** uses 4-bit quantization ‚Üí ~3.5 GB VRAM usage
- **CPU mode** uses full precision ‚Üí ~16-20 GB RAM usage
- **GPU mode appears smaller** because base model is often cached
- **The real win** is memory efficiency, not download size

The documentation should (and now does) make this clearer!

---

**Bottom Line**: When we say GPU mode "only needs adapters", we're being imprecise. What we mean is: "GPU mode downloads the same files but uses 4-bit quantization to fit in much less memory, and you might already have the base model cached."
